#### scrapy框架的基本使用
1. 环境的安装：
   * mac or linux: pip install scrapy
   * 测试：在终端里录入scrapy指令，没有报错即表示安装成功！
2. 创建一个工程：scrapy startproject xxxPro
3. cd xxxPro
4. 在spiders子目录中创建一个爬虫文件
   * scrapy genspider spiderName www.xxt.com
5. 执行工程：
   * scrapy crawl spiderName
6. scrapy数据解析，案例如下
7. scrapy持久化存储
  * 基于终端指令
    * 要求：只可以将parse方法中的返回值存储到本地的文本文件中
    * 注意：持久化存储对应的文本文件的类型只能是：'json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle'
    * 指令：scrapy crawl XX -o filePath
    * 好处：简洁高效便捷
    * 缺点：局限性比较强（数据智能存储到指定后缀的文本文件中）

  * 基于管道
    * 编码流程
      1. 数据解析
      2. 在item类中定义相关的属性
      3. 将解析的数据封装存储到item类型的对象
      4. 将item类型的对象提交给管道进行持久化存储的操作
      5. 在管道类的process_item中要将其接收到的item对象中存储的数据进行持久化存储
      6. 在配置文件中开启管道
    * 好处：通用性强 

#### scrapy数据解析案例
```
# setting.py
1. 配置USER_AGENT
2. ROBOTSTXT_OBEY = False
3. LOG_LEVEL = 'ERROR'

# 主代码
import scrapy


class QiushibkSpider(scrapy.Spider):
    name = 'qiushibk'
    # allowed_domains = ['https://www.qiushibaike.com/article/124996947']
    start_urls = ['https://www.qiushibaike.com/text/']

    def parse(self, response):
         div_list = response.xpath('//div[@class="col1 old-style-col1"]/div')
         for div in div_list:
         
             # xpath返回的是列表，但是列表元素一定是Selector类型的对象
             # extract()可以将selector对象中data参数存储的字符串提取出来
             # 方式1：author = div.xpath('./div[1]/a[2]/h2/text()').extract()
             # 方式2：将列表中第0个元素实行extract操作
             author = div.xpath('./div[1]/a[2]/h2/text()').extract_first()
             
             # 要获取span下面的所有内容，但是span下还有其他标签，此时需要使用'//'2个'/'
             # 列表调用了extract()之后，则表示将列表中每一个selector对象中data所对应的字符串中提取出来
             content = div.xpath('./a[1]/div/span//text()').extract()
             content =''.join(content) # 提取文本内容
             
             print(author, content)
             break

```

#### scrapy持久化存储：基于终端指令
```
import scrapy


class QiushibkSpider(scrapy.Spider):
    name = 'qiushibk'
    # allowed_domains = ['https://www.qiushibaike.com/article/124996947']
    start_urls = ['https://www.qiushibaike.com/text/']

    def parse(self, response):
         div_list = response.xpath('//div[@class="col1 old-style-col1"]/div')
         all_data = []  # 存储所有解析到的数据
         for div in div_list:
             author = div.xpath('./div[1]/a[2]/h2/text()').extract_first()
             content = div.xpath('./a[1]/div/span//text()').extract()
             content =''.join(content) 

             dic = {
                 'author': author,
                 'content': content
             }
             all_data.append(dic)

         return all_data

```

#### scrapy持久化存储：基于管道

```
执行文件

import scrapy
from qiushi.items import QiushiItem

class QiushibkSpider(scrapy.Spider):
    name = 'qiushibk'
    start_urls = ['https://www.qiushibaike.com/text/']

    def parse(self, response):
         div_list = response.xpath('//div[@class="col1 old-style-col1"]/div')
         for div in div_list:
             author = div.xpath('./div[1]/a[2]/h2/text()').extract_first()
             content = div.xpath('./a[1]/div/span//text()').extract()
             content =''.join(content) # 提取文本内容

             item = QiushiItem()
             item['author'] = author
             item['content'] = content

             yield item # 将item提交给了管道
```

```
items.py配置：在item类中定义相关的属性

class QiushiItem(scrapy.Item):
    # define the fields for your item here like:
    author = scrapy.Field()
    content = scrapy.Field()
```
```
pipelines.py配置：在管道类的process_item中要将其接收到的item对象中存储的数据进行持久化存储

class QiushiPipeline:
    fp = None
    # 重写父类的一个方法：该方法只在开始爬虫的时候被调用1次
    def open_spider(self, spider):
        print('开始爬虫……')
        self.fp = open('/Users/js/PycharmProjects/爬虫课程/第七章：scrapy框架/qiushi/qiubai.txt','w', encoding='utf-8')
        
    # 专门用来处理item类型对象
    # 该方法可以接收爬虫文件中提交过来的item对象
    # 该方法每接收一个item就会被调用一次
    def process_item(self, item, spider):
        author = item['author']
        content = item['content']
        self.fp.write(author+':'+content+'\n')

        return item
 
    # 重写父类的一个方法：该方法只在结束爬虫的时候被调用1次
    def close_spider(self, spider):
        print('结束爬虫！')
        self.fp.close()

```
```
setting.py配置

# 数据解析的配置
USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.55 Safari/537.36'
ROBOTSTXT_OBEY = False
LOG_LEVEL = 'ERROR'

# 在配置文件中开启管道
ITEM_PIPELINES = {
    # 300表示的是优先级，数值越小，优先级越高
   'qiushi.pipelines.QiushiPipeline': 300
}
```

#### 面试题：一份存到本地，一份存到数据库
