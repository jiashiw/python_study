> 聚焦爬虫：爬取页面中指定的页面内容

### 编码流程
1. 指定URL
2. 发起请求
3. 获取相应数据
4. **数据解析**
5. 持久化存储

### 数据解析分类
* 正则
* bs4
* xpath(***)

### 1.数据解析原理-通用
* 解析的局部的文本内容都会在标签之间或者标签对应的属性中进行存储
* 1.进行指定标签的定位
* 2.标签或标签对应的属性中存储的数据进行提取（解析）

```
需求：用正则表达式提取img src后的图片链接
原代码：
    < div class ="thumb" >

    < a href = "/article/124596381" target = "_blank" >
    < img src = "//pic.qiushibaike.com/system/pictures/12459/124596381/medium/P3DV2L54GXE14N0X.jpg" alt = "糗事#124596381" class ="illustration" width="100%" height="auto" >
    < / a >
    
    < / div >

正则表达式解析：
ex = '<div class="thumb">.*?<img src="(.*?)" alt.*?</div>'
其中（.*?）代替的就是img scr的链接
```

### 2.数据解析原理-bs4
* 在通用原理基础上，python专用方法
* 1.实例化一个beautifulsoup对象，并且将页面源码数据加载到该对象中
* 2.通过调用beautifulsoup对象中相关属性或者方法进行标签定位和数据提取

* bs4的用法总结
```
1.bs4两种写法:
    本地文件：soup = BeautifulSoup(open('本地文件')，'lxml') 
    网络爬取请求做的：soup = BeautifulSop(('对象名字'),'lxml')

2.根据标签名进行查找:
    soup.a 只能查找第一个符合条件的属性标签

3.获取的属性:
    soup.a.attrs 获取所有的属性和值，返回一个字典
    soup.a.attrs['href'] 获取href的属性
    soup.a['href'] 同上

4.获取内容:
    soup.a.text 建议使用这种
    soup.a.string
    soup.a.get_text

5.find的使用方法:
    · soup.find('a') 找到第一个符合a的标签，等同于soup.a
    · 属性定位：
        - soup.find('div', class_/id/sttr='song)

6.find_all的使用方法：
    It = soup.find_all('tagName') 返回符合要求的所有标签（列表）
    
7.select选择器的使用方法:
    根据选择器选择指定的内容，返回的是一个列表
    常见的选择器：标签选择器，类选择器，ID选择器，组合选择器，层级选择器，伪类选择器，属性选择器
    传递给select（)方法的选择器

        soup.select('div')                   所有名为<div>的元素
        soup.select('#author')               带有id属性为author的元素
        soup.select('.notice')               所有使用CSS class 属性名为notice的元素
        soup.select('div span')              所有在<div>元素之内的<span>元素：：“ ”空格表示的是多个层级，与>比
        soup.select('div > span')            所有直接在<div>元素之内的<span>元素，中间没有其他元素：>表示的是一个层级，与“ ”相比
        soup.select('input[name]')           所有名为<input>,并有一个name属性，其值无所谓的元素
        soup.select('input[type="button"]')  所有名为<input>,并有一个type属性，其值为bottom的元素
    ————————————————
    原文链接：https://blog.csdn.net/weixin_46244909/article/details/104210838

    获取标签之间的文本
    - soup.a.text/get_text()：获得标签下面的所有文本内容，即使不是直系标签
    - soup.a.string：只可以获得该标签下面的直系内容
    获取标签中属性值
    - soup.a['href']
```

### 3.xpath解析：最常用且最便捷高效的一种解析方式。通用性

* xpath解析原理
    * 实例化1个etree的对象，且需要被解析的页面源码数据加载到该对象
    * 调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获

* 如何实例化1个etree对象：from lxml import etree
    1. 将本地的html文档中的源码数据加载到etree对象中
    etree.parse(filePath)
    2. 可以将从互联网上获取的源码数据加载到该对象中
    etree.HTML('page_text')
    3. xpath('xpath表达式')

* xpath表达式：
    1. /：表示的是从根节点开始定位。表示的是一个层级
    2. //：表示的是多个层级。可以表示从任意位置开始定位
    3. 属性定位：//div[@class="song"]；通用写法：tag[@attr=""]
    4. 索引定位：//div[@class="song"]/p[3]，索引从1开始
    5. 取文本：
        * /text()，获取标签中直系的文本内容
        * //text()，获取标签中非直系的文本内容
    7. 取属性：
        * /@sttrName，如:img/@src


